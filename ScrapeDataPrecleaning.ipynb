{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import DCDSFunctions as f\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Scrape records of application for H1B visa applications from h1bdata.info </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H1BDATA.info\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2014', '2015', '2016', '2017', '2018', '2019']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample query\n",
    "years = list(range(2014,2020))\n",
    "years = [str(y) for y in years]\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data+scientist',\n",
       " 'senior+data+scientist',\n",
       " 'associate+data+scientist',\n",
       " 'data+scientist+ii',\n",
       " 'data+scientist+i',\n",
       " 'sr.+data+scientist',\n",
       " 'lead+data+scientist',\n",
       " 'data+science+engineer',\n",
       " 'data+science+analyst',\n",
       " 'senior+associate,+data+science']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape Job Title Information\n",
    "\n",
    "job_title_url = 'https://h1bdata.info/topjobs.php'\n",
    "title_resp = requests.get(job_title_url)\n",
    "\n",
    "titles_all = BeautifulSoup(title_resp.content, 'html.parser') # soup type\n",
    "title_block = titles_all.find('tbody') # element.Tag\n",
    "title_rows = title_block.findAll('tr') # element.ResultSet which is a list of Tags\n",
    "title_links= [x.findAll('a') for x in title_rows] # list of list each with one element.Tag\n",
    "title_data = [x[0].get_text()  for x in title_links if x]\n",
    "\n",
    "DS_titles = [x.lower().replace(' ','+') for x in title_data if 'DATA SCI' in x]\n",
    "DS_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Save Path Keywords based on DS_titles above\n",
    "save_titles= ['DSgen','SeniorDS','AssocDS','DSII','DSI','SrDS','LeadDS','DSEng','DSAnalyst','SenAssocDS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Job Information and Pickle\n",
    "\n",
    "for ind,title in enumerate(DS_titles):\n",
    "    save_title = save_titles[ind]\n",
    "\n",
    "    for year in years:\n",
    "        h1b1_url = 'https://h1bdata.info/index.php?em=&job='+title+'&city=&year='+year\n",
    "\n",
    "#         resp = requests.get(h1b1_url)\n",
    "\n",
    "        h1b1_annual = BeautifulSoup(resp.content, 'html.parser')\n",
    "        block = h1b1_annual.find('tbody')\n",
    "        data = block.findAll('tr')\n",
    "        t = [x.findAll('td') for x in data]\n",
    "\n",
    "        col = ['Company','Role','Salary','Location','SubmitDate','StartDate','Status']\n",
    "        ls = []\n",
    "        for row in t:\n",
    "            vals = [x.text for x in row]\n",
    "            dic = dict(zip(col,vals))\n",
    "            ls.append(dic)\n",
    "\n",
    "        h1b1 = pd.DataFrame(ls)\n",
    "        pause = random.randrange(3,15)\n",
    "        print('Pausing for ',pause,' seconds...')\n",
    "        time.sleep(pause)\n",
    "\n",
    "        filename = 'data/h1b1data_'+save_title+'_'+year+'.p'\n",
    "        outfile = open(filename,'wb')\n",
    "        pickle.dump(h1b1,outfile)\n",
    "        outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickles and merge to one DataFrame\n",
    "\n",
    "h1b1_all = pd.DataFrame()\n",
    "\n",
    "for ind,title in enumerate(save_titles):\n",
    "    \n",
    "    for i, year in enumerate(years):\n",
    "        \n",
    "        filename = 'data/h1b1data_'+title+'_'+year+'.p'\n",
    "        infile = open(filename,'rb')\n",
    "        t = pickle.load(infile)\n",
    "        t['Year'] = year\n",
    "\n",
    "        if ((ind == 0) & (i == 0)):\n",
    "            h1b1_all = t\n",
    "        elif sum(t.shape)>=1:\n",
    "            h1b1_all = pd.concat([h1b1_all,t], ignore_index=True, sort=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "h1b1_all.to_csv('data/dirtydata.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
